{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:21:25.310349Z",
     "start_time": "2025-03-28T04:17:26.518939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from ultralytics import YOLO\n",
    "from shutil import move\n",
    "\n",
    "# Configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_SIZE = 640\n",
    "SEGMENT_DURATION = 600  # 10 minutes in seconds\n",
    "PROGRESS_INTERVAL = 45  # Seconds between updates\n",
    "BBOX_PADDING = 0.15\n",
    "FRAME_SKIP = 1\n",
    "MIN_SEGMENT_LENGTH = 300\n",
    "\n",
    "\n",
    "def _get_video_duration(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "    return frames / fps if fps else 0\n",
    "\n",
    "\n",
    "def _create_writer(base_name, timestamp, output_dir, segment_num, fps):\n",
    "   \"\"\"Create video writer with standardized naming\"\"\"\n",
    "   output_path = os.path.join(\n",
    "       output_dir,\n",
    "       f\"{base_name}_{timestamp}_part{segment_num:03d}.mp4\"\n",
    "   )\n",
    "   return cv2.VideoWriter(\n",
    "       output_path,\n",
    "       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "       fps,\n",
    "       (MODEL_SIZE, MODEL_SIZE)\n",
    "   )\n",
    "\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self):\n",
    "        self.model = YOLO('referee17february.pt').to(DEVICE)\n",
    "        self.model.fuse()\n",
    "        if DEVICE == 'cuda':\n",
    "            self.model.half()\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.class_id = self._get_referee_class_id()\n",
    "\n",
    "    def _get_referee_class_id(self):\n",
    "        return list(self.model.names.keys())[\n",
    "            list(self.model.names.values()).index('referee')\n",
    "        ]\n",
    "\n",
    "    def process_videos(self, input_dir='dataVideo', output_dir='forLabel', used_dir='used'):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(used_dir, exist_ok=True)\n",
    "\n",
    "        for video_file in [f for f in os.listdir(input_dir) if f.endswith('.mp4')]:\n",
    "            video_path = os.path.join(input_dir, video_file)\n",
    "            print(f\"Starting processing: {video_file}\")\n",
    "            try:\n",
    "                self._process_single_video(video_path, output_dir, used_dir)\n",
    "                print(f\"Completed processing: {video_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_file}: {str(e)}\")\n",
    "\n",
    "    def _process_single_video(self, video_path, output_dir, used_dir):\n",
    "        if _get_video_duration(video_path) > 3600:\n",
    "            print(\"Splitting video into 1-hour chunks...\")\n",
    "            chunks = self._split_into_hourly_chunks(video_path, output_dir)\n",
    "            print(f\"Successfully split into {len(chunks)} chunks\")\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                print(f\"Processing chunk: {os.path.basename(chunk)}\")\n",
    "                self._process_chunk(chunk, output_dir)\n",
    "                print(f\"Completed chunk: {os.path.basename(chunk)}\")\n",
    "                os.remove(chunk)  # Remove temporary chunk file\n",
    "            \n",
    "            self._safe_move(video_path, used_dir)\n",
    "            print(f\"Moved original video to: {used_dir}\")\n",
    "            return\n",
    "\n",
    "        print(\"Processing single video chunk...\")\n",
    "        self._process_chunk(video_path, output_dir)\n",
    "        self._safe_move(video_path, used_dir)\n",
    "        print(f\"Moved original video to: {used_dir}\")\n",
    "\n",
    "    def _process_frame(self, frame, last_valid):\n",
    "        \"\"\"Process frame with detection and suppression of YOLO outputs\"\"\"\n",
    "        # Convert to model-compatible format\n",
    "        h, w = frame.shape[:2]\n",
    "        new_h = (h // 32) * 32\n",
    "        new_w = (w // 32) * 32\n",
    "        resized = cv2.resize(frame, (new_w, new_h))\n",
    "        \n",
    "        # Prepare tensor (BCHW format)\n",
    "        tensor = torch.from_numpy(resized).to(DEVICE)\n",
    "        tensor = tensor.permute(2, 0, 1)\n",
    "        tensor = tensor.half() if DEVICE == 'cuda' else tensor.float()\n",
    "        tensor = (tensor / 255.0).unsqueeze(0)\n",
    "\n",
    "        # Run inference with suppressed outputs\n",
    "        with torch.no_grad():\n",
    "            results = self.model(tensor, verbose=False)[0]\n",
    "\n",
    "        # Process results with padding\n",
    "        detected = False\n",
    "        if len(results.boxes.xyxy) > 0:\n",
    "            detected = True\n",
    "            x1, y1, x2, y2 = self._padded_coords(results.boxes.xyxy[0].cpu().numpy(), frame.shape)\n",
    "            cropped = frame[y1:y2, x1:x2]\n",
    "            return cv2.resize(cropped, (MODEL_SIZE, MODEL_SIZE)), detected\n",
    "        return last_valid, detected\n",
    "    def _padded_coords(self, coords, frame_shape):\n",
    "        \"\"\"Calculate padded coordinates with boundary checks\"\"\"\n",
    "        h, w = frame_shape[:2]\n",
    "        pad_x = int((coords[2]-coords[0])*BBOX_PADDING)\n",
    "        pad_y = int((coords[3]-coords[1])*BBOX_PADDING)\n",
    "        return (\n",
    "            max(0, int(coords[0])-pad_x),\n",
    "            max(0, int(coords[1])-pad_y),\n",
    "            min(w, int(coords[2])+pad_x),\n",
    "            min(h, int(coords[3])+pad_y)\n",
    "        )\n",
    "    def _process_chunk(self, video_path, output_dir):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Processing setup\n",
    "        writer = None\n",
    "        last_valid = None\n",
    "        segment_num = 1\n",
    "        segment_start_time = 0\n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        last_progress_update = time.time()\n",
    "        initial_detection = False\n",
    "        detections_in_interval = 0\n",
    "\n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                # Skip frames but maintain temporal accuracy\n",
    "                for _ in range(FRAME_SKIP):\n",
    "                    if not cap.grab():\n",
    "                        break\n",
    "                \n",
    "                ret, frame = cap.retrieve()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000  # Current time in seconds\n",
    "\n",
    "                # Process frame and get detection\n",
    "                processed_frame, detected = self._process_frame(frame, last_valid)\n",
    "                if detected:\n",
    "                    detections_in_interval += 1\n",
    "                \n",
    "                if processed_frame is not None:\n",
    "                    if not initial_detection:\n",
    "                        initial_detection = True\n",
    "                        segment_start_time = current_time\n",
    "                        writer = _create_writer(base_name, timestamp, output_dir, segment_num, orig_fps)\n",
    "\n",
    "                    # Start new segment every 10 minutes\n",
    "                    if current_time - segment_start_time >= SEGMENT_DURATION:\n",
    "                        writer.release()\n",
    "                        segment_num += 1\n",
    "                        writer = _create_writer(base_name, timestamp, output_dir, segment_num, orig_fps)\n",
    "                        segment_start_time = current_time\n",
    "\n",
    "                    # Write processed frame\n",
    "                    writer.write(processed_frame)\n",
    "                    last_valid = processed_frame\n",
    "\n",
    "                # Progress reporting\n",
    "                if time.time() - last_progress_update >= PROGRESS_INTERVAL:\n",
    "                    elapsed = timedelta(seconds=int(current_time))\n",
    "                    pct = (current_time / _get_video_duration(video_path)) * 100\n",
    "                    print(f\"[{elapsed}] {pct:.1f}% processed | Segments: {segment_num} | Detections: {detections_in_interval}\")\n",
    "                    detections_in_interval = 0\n",
    "                    last_progress_update = time.time()\n",
    "\n",
    "        finally:\n",
    "            # Handle final segment\n",
    "            if writer:\n",
    "                final_duration = current_time - segment_start_time\n",
    "                if final_duration >= MIN_SEGMENT_LENGTH:\n",
    "                    writer.release()\n",
    "                else:\n",
    "                    writer.release()\n",
    "                    os.remove(writer.filename)\n",
    "            cap.release()\n",
    "\n",
    "    def _split_into_hourly_chunks(self, video_path, output_dir):\n",
    "        \"\"\"Split video into 1-hour chunks with cleanup\"\"\"\n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_pattern = os.path.join(output_dir, f\"{base_name}_{timestamp}_temp_part%03d.mp4\")\n",
    "        \n",
    "        try:\n",
    "            subprocess.run([\n",
    "                'ffmpeg', '-i', video_path,\n",
    "                '-c', 'copy',\n",
    "                '-map', '0',\n",
    "                '-segment_time', '01:00:00',\n",
    "                '-f', 'segment',\n",
    "                '-reset_timestamps', '1',\n",
    "                '-loglevel', 'error',\n",
    "                output_pattern\n",
    "            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error splitting video: {e.stderr.decode() if e.stderr else 'Unknown error'}\")\n",
    "            raise\n",
    "\n",
    "        return sorted([\n",
    "            os.path.join(output_dir, f) \n",
    "            for f in os.listdir(output_dir) \n",
    "            if f.startswith(f\"{base_name}_{timestamp}_temp\")\n",
    "        ])\n",
    "    \n",
    "    def _safe_move(self, src, dest_dir):\n",
    "        \"\"\"Improved moving with resource cleanup\"\"\"\n",
    "        dest = os.path.join(dest_dir, os.path.basename(src))\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                # Ensure no open handles\n",
    "                if os.path.exists(src):\n",
    "                    if attempt > 0:\n",
    "                        time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                    if os.path.exists(dest):\n",
    "                        os.remove(dest)\n",
    "                    move(src, dest)\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                if attempt == 4:\n",
    "                    print(f\"Failed to move file after 5 attempts: {str(e)}\")\n",
    "                    raise\n",
    "        print(f\"Unexpected error moving file: {src}\")\n",
    "\n",
    "    # ... [keep other existing methods] ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = VideoProcessor()\n",
    "    print(\"Starting video processing pipeline...\")\n",
    "    processor.process_videos()\n",
    "    print(\"All processing completed!\")\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11x summary (fused): 464 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "Starting video processing pipeline...\n",
      "Starting processing: videoSplitted.mp4\n",
      "Processing single video chunk...\n",
      "[0:00:15] 1.2% processed | Segments: 1 | Detections: 0\n",
      "[0:00:32] 2.6% processed | Segments: 1 | Detections: 0\n",
      "[0:00:48] 3.9% processed | Segments: 1 | Detections: 0\n",
      "[0:01:04] 5.2% processed | Segments: 1 | Detections: 0\n",
      "[0:01:20] 6.5% processed | Segments: 1 | Detections: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 247\u001B[0m\n\u001B[0;32m    245\u001B[0m processor \u001B[38;5;241m=\u001B[39m VideoProcessor()\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting video processing pipeline...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 247\u001B[0m processor\u001B[38;5;241m.\u001B[39mprocess_videos()\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll processing completed!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 64\u001B[0m, in \u001B[0;36mVideoProcessor.process_videos\u001B[1;34m(self, input_dir, output_dir, used_dir)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting processing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_single_video(video_path, output_dir, used_dir)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompleted processing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "Cell \u001B[1;32mIn[1], line 86\u001B[0m, in \u001B[0;36mVideoProcessor._process_single_video\u001B[1;34m(self, video_path, output_dir, used_dir)\u001B[0m\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProcessing single video chunk...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_chunk(video_path, output_dir)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_safe_move(video_path, used_dir)\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMoved original video to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mused_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 156\u001B[0m, in \u001B[0;36mVideoProcessor._process_chunk\u001B[1;34m(self, video_path, output_dir)\u001B[0m\n\u001B[0;32m    153\u001B[0m current_time \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mget(cv2\u001B[38;5;241m.\u001B[39mCAP_PROP_POS_MSEC) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1000\u001B[39m  \u001B[38;5;66;03m# Current time in seconds\u001B[39;00m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;66;03m# Process frame and get detection\u001B[39;00m\n\u001B[1;32m--> 156\u001B[0m processed_frame, detected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_frame(frame, last_valid)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m detected:\n\u001B[0;32m    158\u001B[0m     detections_in_interval \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[1;32mIn[1], line 106\u001B[0m, in \u001B[0;36mVideoProcessor._process_frame\u001B[1;34m(self, frame, last_valid)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;66;03m# Run inference with suppressed outputs\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 106\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(tensor, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    108\u001B[0m \u001B[38;5;66;03m# Process results with padding\u001B[39;00m\n\u001B[0;32m    109\u001B[0m detected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:179\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, source, stream, **kwargs)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    152\u001B[0m     source: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28mint\u001B[39m, Image\u001B[38;5;241m.\u001B[39mImage, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray, torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    153\u001B[0m     stream: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    155\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m    156\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(source, stream, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:557\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, source, stream, predictor, **kwargs)\u001B[0m\n\u001B[0;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_prompts\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[0;32m    556\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mset_prompts(prompts)\n\u001B[1;32m--> 557\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mpredict_cli(source\u001B[38;5;241m=\u001B[39msource) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor(source\u001B[38;5;241m=\u001B[39msource, stream\u001B[38;5;241m=\u001B[39mstream)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:173\u001B[0m, in \u001B[0;36mBasePredictor.__call__\u001B[1;34m(self, source, model, stream, *args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m---> 36\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     40\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:258\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[1;34m(self, source, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m    255\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(im0s)\n\u001B[0;32m    257\u001B[0m \u001B[38;5;66;03m# Inference\u001B[39;00m\n\u001B[1;32m--> 258\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[38;5;241m1\u001B[39m]:\n\u001B[0;32m    259\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minference(im, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39membed:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\ops.py:51\u001B[0m, in \u001B[0;36mProfile.__exit__\u001B[1;34m(self, type, value, traceback)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mtype\u001B[39m, value, traceback):  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Stop timing.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart  \u001B[38;5;66;03m# delta-time\u001B[39;00m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mt \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdt\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\ops.py:61\u001B[0m, in \u001B[0;36mProfile.time\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Get current time.\"\"\"\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcuda:\n\u001B[1;32m---> 61\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:954\u001B[0m, in \u001B[0;36msynchronize\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    952\u001B[0m _lazy_init()\n\u001B[0;32m    953\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice(device):\n\u001B[1;32m--> 954\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_synchronize()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
