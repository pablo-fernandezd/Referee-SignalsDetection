{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:48:47.585268Z",
     "start_time": "2025-03-28T05:47:29.268974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Added/modified parts are marked with comments\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "from shutil import move\n",
    "\n",
    "# Configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_SIZE = 640\n",
    "SEGMENT_DURATION = 600\n",
    "PROGRESS_INTERVAL = 45\n",
    "BBOX_PADDING = 0.15\n",
    "FRAME_SKIP = 3\n",
    "MIN_SEGMENT_LENGTH = 300\n",
    "MIN_CONFIDENCE = 0.6  # Added confidence threshold\n",
    "HISTORY_BUFFER_SIZE = 15  # Frames to remember for position consensus\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self):\n",
    "        self.model = YOLO('referee17february.pt').to(DEVICE)\n",
    "        self.model.fuse()\n",
    "        if DEVICE == 'cuda':\n",
    "            self.model.half()\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.class_id = self._get_referee_class_id()\n",
    "        self.detection_history = deque(maxlen=HISTORY_BUFFER_SIZE)  # Track recent positions\n",
    "\n",
    "    def _get_referee_class_id(self):\n",
    "        \"\"\"Get class ID for 'referee' from model metadata\"\"\"\n",
    "        return list(self.model.names.keys())[\n",
    "            list(self.model.names.values()).index('referee')\n",
    "        ]\n",
    "\n",
    "    def process_videos(self, input_dir='dataVideo', output_dir='forLabel', used_dir='used'):\n",
    "        \"\"\"Main processing pipeline for all videos in input directory\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(used_dir, exist_ok=True)\n",
    "\n",
    "        for video_file in [f for f in os.listdir(input_dir) if f.endswith('.mp4')]:\n",
    "            video_path = os.path.join(input_dir, video_file)\n",
    "            print(f\"\\nüöÄ Starting processing: {video_file}\")\n",
    "            try:\n",
    "                self._process_single_video(video_path, output_dir, used_dir)\n",
    "                print(f\"\\n‚úÖ Successfully processed: {video_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error processing {video_file}: {str(e)}\")\n",
    "\n",
    "    def _process_single_video(self, video_path, output_dir, used_dir):\n",
    "        \"\"\"Process individual video file with split/processing logic\"\"\"\n",
    "        duration = self._get_video_duration(video_path)\n",
    "        \n",
    "        if duration > 3600:  # Split videos longer than 1 hour\n",
    "            print(\"‚è≥ Splitting into 1-hour chunks...\")\n",
    "            chunks = self._split_into_hourly_chunks(video_path, output_dir)\n",
    "            print(f\"üì¶ Created {len(chunks)} temporary chunks\")\n",
    "            \n",
    "            # Process each chunk separately\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                print(f\"\\nüîß Processing chunk {i}/{len(chunks)}\")\n",
    "                self._process_video_chunk(chunk, output_dir)\n",
    "                os.remove(chunk)\n",
    "                print(f\"üßπ Cleaned temporary chunk {i}\")\n",
    "            \n",
    "            self._safe_move(video_path, used_dir)\n",
    "            print(f\"\\nüì¶ Moved original to: {used_dir}\")\n",
    "            return\n",
    "\n",
    "        # Direct processing for short videos\n",
    "        print(\"üîç Processing single video chunk...\")\n",
    "        self._process_video_chunk(video_path, output_dir)\n",
    "        self._safe_move(video_path, used_dir)\n",
    "        print(f\"\\nüì¶ Moved original to: {used_dir}\")\n",
    "\n",
    "    def _process_video_chunk(self, video_path, output_dir):\n",
    "        \"\"\"Core video processing with segmentation and tracking\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Processing state\n",
    "        writer = None\n",
    "        last_valid = None\n",
    "        segment_num = 1\n",
    "        segment_start_time = 0\n",
    "        last_progress_update = time.time()\n",
    "        detections = 0\n",
    "        frame_counter = 0\n",
    "\n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                # Frame skipping for performance\n",
    "                for _ in range(FRAME_SKIP):\n",
    "                    if not cap.grab():\n",
    "                        break\n",
    "                \n",
    "                ret, frame = cap.retrieve()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "                frame_counter += 1\n",
    "\n",
    "                # Process frame with detection/tracking\n",
    "                processed_frame, detected = self._process_frame(frame, last_valid)\n",
    "                \n",
    "                if processed_frame is not None:\n",
    "                    # Initialize or update video writer\n",
    "                    if writer is None or (current_time - segment_start_time >= SEGMENT_DURATION):\n",
    "                        if writer:\n",
    "                            writer.release()\n",
    "                            print(f\"üíæ Saved segment {segment_num} ({timedelta(seconds=int(current_time - segment_start_time))})\")\n",
    "                        segment_num += 1\n",
    "                        writer = self._create_writer(base_name, timestamp, output_dir, segment_num, orig_fps)\n",
    "                        segment_start_time = current_time\n",
    "                        print(f\"üé¨ Started segment {segment_num} at {timedelta(seconds=int(segment_start_time))}\")\n",
    "\n",
    "                    writer.write(processed_frame)\n",
    "                    last_valid = processed_frame\n",
    "                    if detected:\n",
    "                        detections += 1\n",
    "\n",
    "                # Progress reporting\n",
    "                if time.time() - last_progress_update >= PROGRESS_INTERVAL:\n",
    "                    elapsed = timedelta(seconds=int(current_time))\n",
    "                    pct = (current_time / self._get_video_duration(video_path)) * 100\n",
    "                    print(\n",
    "                        f\"\\n‚è±Ô∏è Progress [{elapsed}] \"\n",
    "                        f\"{pct:.1f}% complete | \"\n",
    "                        f\"Segments: {segment_num} | \"\n",
    "                        f\"Recent detections: {detections}\"\n",
    "                    )\n",
    "                    last_progress_update = time.time()\n",
    "                    detections = 0\n",
    "\n",
    "        finally:\n",
    "            # Final segment handling\n",
    "            if writer:\n",
    "                final_duration = current_time - segment_start_time\n",
    "                if final_duration >= MIN_SEGMENT_LENGTH:\n",
    "                    writer.release()\n",
    "                    print(f\"üíæ Saved final segment {segment_num} ({timedelta(seconds=int(final_duration))})\")\n",
    "                else:\n",
    "                    writer.release()\n",
    "                    os.remove(writer.filename)\n",
    "                    print(f\"üßπ Removed short segment (<{MIN_SEGMENT_LENGTH}s)\")\n",
    "            cap.release()\n",
    "\n",
    "    def _process_frame(self, frame, last_valid):\n",
    "        \"\"\"Process frame with detection consensus and quality preservation\"\"\"\n",
    "        original_frame = frame.copy()\n",
    "        \n",
    "        # Get detection with highest confidence that meets threshold\n",
    "        resized_frame = cv2.resize(frame, (MODEL_SIZE, MODEL_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "        tensor = torch.from_numpy(resized_frame).to(DEVICE)\n",
    "        tensor = tensor.permute(2, 0, 1).float() / 255.0\n",
    "        if DEVICE == 'cuda':\n",
    "            tensor = tensor.half()\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = self.model(tensor, verbose=False)[0]\n",
    "\n",
    "        best_bbox = None\n",
    "        if len(results.boxes) > 0:\n",
    "            # Filter by confidence and get best detection\n",
    "            valid_detections = [box for box in results.boxes.data if box[4] >= MIN_CONFIDENCE]\n",
    "            if valid_detections:\n",
    "                best_detection = max(valid_detections, key=lambda x: x[4])\n",
    "                best_bbox = best_detection[:4].cpu().numpy()\n",
    "                self.detection_history.append(best_bbox)  # Add to history buffer\n",
    "\n",
    "        # Use most frequent recent detection for stability\n",
    "        current_bbox = self._get_consensus_bbox()\n",
    "        \n",
    "        if current_bbox is not None:\n",
    "            # Convert coordinates to original frame scale\n",
    "            x_scale = frame.shape[1] / MODEL_SIZE\n",
    "            y_scale = frame.shape[0] / MODEL_SIZE\n",
    "            \n",
    "            # Apply padding relative to detection size\n",
    "            bbox_width = current_bbox[2] - current_bbox[0]\n",
    "            bbox_height = current_bbox[3] - current_bbox[1]\n",
    "            pad_x = int(bbox_width * BBOX_PADDING)\n",
    "            pad_y = int(bbox_height * BBOX_PADDING)\n",
    "            \n",
    "            # Calculate coordinates with boundary checks\n",
    "            x1 = max(0, int(current_bbox[0] * x_scale) - pad_x)\n",
    "            y1 = max(0, int(current_bbox[1] * y_scale) - pad_y)\n",
    "            x2 = min(frame.shape[1], int(current_bbox[2] * x_scale) + pad_x)\n",
    "            y2 = min(frame.shape[0], int(current_bbox[3] * y_scale) + pad_y)\n",
    "            \n",
    "            # Crop and resize using original frame data for better quality\n",
    "            cropped = original_frame[y1:y2, x1:x2]\n",
    "            return cv2.resize(cropped, (MODEL_SIZE, MODEL_SIZE), interpolation=cv2.INTER_LINEAR), True\n",
    "        \n",
    "        # Fallback to last valid detection if available\n",
    "        return last_valid, False\n",
    "\n",
    "    def _get_consensus_bbox(self):\n",
    "        \"\"\"Get most common recent bbox from history buffer\"\"\"\n",
    "        if not self.detection_history:\n",
    "            return None\n",
    "            \n",
    "        # Find most frequent bbox position using histogram\n",
    "        bbox_counts = {}\n",
    "        for bbox in self.detection_history:\n",
    "            key = tuple(map(int, bbox))\n",
    "            bbox_counts[key] = bbox_counts.get(key, 0) + 1\n",
    "        \n",
    "        if bbox_counts:\n",
    "            return max(bbox_counts.items(), key=lambda x: x[1])[0]\n",
    "        return None\n",
    "\n",
    "\n",
    "    def _padded_coords(self, coords, shape):\n",
    "        \"\"\"Calculate padded coordinates with boundary checks\"\"\"\n",
    "        h, w = shape[:2]\n",
    "        dx = int((coords[2]-coords[0])*BBOX_PADDING)\n",
    "        dy = int((coords[3]-coords[1])*BBOX_PADDING)\n",
    "        return (\n",
    "            max(0, int(coords[0])-dx),\n",
    "            max(0, int(coords[1])-dy),\n",
    "            min(w, int(coords[2])+dx),\n",
    "            min(h, int(coords[3])+dy)\n",
    "        )\n",
    "\n",
    "    def _create_writer(self, base_name, timestamp, output_dir, segment_num, fps):\n",
    "        \"\"\"Create video writer with standardized naming\"\"\"\n",
    "        output_path = os.path.join(\n",
    "            output_dir,\n",
    "            f\"{base_name}_{timestamp}_part{segment_num:03d}.mp4\"\n",
    "        )\n",
    "        return cv2.VideoWriter(\n",
    "            output_path,\n",
    "            cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "            fps,\n",
    "            (MODEL_SIZE, MODEL_SIZE)\n",
    "        )\n",
    "\n",
    "    def _split_into_hourly_chunks(self, video_path, output_dir):\n",
    "        \"\"\"Split long videos using FFmpeg\"\"\"\n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_pattern = os.path.join(output_dir, f\"{base_name}_{timestamp}_temp_part%03d.mp4\")\n",
    "        \n",
    "        try:\n",
    "            subprocess.run([\n",
    "                'ffmpeg', '-i', video_path,\n",
    "                '-c', 'copy',\n",
    "                '-map', '0',\n",
    "                '-segment_time', '01:00:00',\n",
    "                '-f', 'segment',\n",
    "                '-reset_timestamps', '1',\n",
    "                '-loglevel', 'error',\n",
    "                output_pattern\n",
    "            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå FFmpeg error: {e.stderr.decode()}\")\n",
    "            raise\n",
    "\n",
    "        return sorted([\n",
    "            os.path.join(output_dir, f) \n",
    "            for f in os.listdir(output_dir) \n",
    "            if f.startswith(f\"{base_name}_{timestamp}_temp\")\n",
    "        ])\n",
    "\n",
    "    def _get_video_duration(self, video_path):\n",
    "        \"\"\"Get video duration in seconds\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        cap.release()\n",
    "        return frames / fps if fps else 0\n",
    "\n",
    "    def _safe_move(self, src, dest_dir):\n",
    "        \"\"\"Atomic file move with retry logic\"\"\"\n",
    "        dest = os.path.join(dest_dir, os.path.basename(src))\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                if os.path.exists(dest):\n",
    "                    os.remove(dest)\n",
    "                move(src, dest)\n",
    "                print(f\"üì§ Moved {os.path.basename(src)} successfully\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                if attempt == 4:\n",
    "                    print(f\"‚ùå Failed to move {os.path.basename(src)}: {str(e)}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = VideoProcessor()\n",
    "    print(\"üöÄ Starting video processing pipeline...\")\n",
    "    processor.process_videos()\n",
    "    print(\"\\nüéâ All processing completed successfully!\")\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11x summary (fused): 464 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "üöÄ Starting video processing pipeline...\n",
      "\n",
      "üöÄ Starting processing: videoSplitted.mp4\n",
      "üîç Processing single video chunk...\n",
      "üé¨ Started segment 2 at 0:00:00\n",
      "\n",
      "‚è±Ô∏è Progress [0:00:45] 3.6% complete | Segments: 2 | Recent detections: 0\n",
      "\n",
      "‚è±Ô∏è Progress [0:01:30] 7.2% complete | Segments: 2 | Recent detections: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 240\u001B[0m\n\u001B[0;32m    238\u001B[0m processor \u001B[38;5;241m=\u001B[39m VideoProcessor()\n\u001B[0;32m    239\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124müöÄ Starting video processing pipeline...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 240\u001B[0m processor\u001B[38;5;241m.\u001B[39mprocess_videos()\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124müéâ All processing completed successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[2], line 47\u001B[0m, in \u001B[0;36mVideoProcessor.process_videos\u001B[1;34m(self, input_dir, output_dir, used_dir)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124müöÄ Starting processing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_single_video(video_path, output_dir, used_dir)\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m‚úÖ Successfully processed: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "Cell \u001B[1;32mIn[2], line 71\u001B[0m, in \u001B[0;36mVideoProcessor._process_single_video\u001B[1;34m(self, video_path, output_dir, used_dir)\u001B[0m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124müîç Processing single video chunk...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_video_chunk(video_path, output_dir)\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_safe_move(video_path, used_dir)\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124müì¶ Moved original to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mused_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[2], line 101\u001B[0m, in \u001B[0;36mVideoProcessor._process_video_chunk\u001B[1;34m(self, video_path, output_dir)\u001B[0m\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m current_written_frames \u001B[38;5;241m%\u001B[39m FRAME_SKIP \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 101\u001B[0m     processed_frame, detected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_frame(frame, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_valid_bbox)\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m detected:\n\u001B[0;32m    103\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_valid_bbox \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_consensus_bbox(frame\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[1;32mIn[2], line 147\u001B[0m, in \u001B[0;36mVideoProcessor._process_frame\u001B[1;34m(self, frame, last_valid)\u001B[0m\n\u001B[0;32m    144\u001B[0m tensor \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 147\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(tensor, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(results\u001B[38;5;241m.\u001B[39mboxes\u001B[38;5;241m.\u001B[39mxyxy) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    150\u001B[0m     valid_detections \u001B[38;5;241m=\u001B[39m [box \u001B[38;5;28;01mfor\u001B[39;00m box \u001B[38;5;129;01min\u001B[39;00m results\u001B[38;5;241m.\u001B[39mboxes\u001B[38;5;241m.\u001B[39mdata \n\u001B[0;32m    151\u001B[0m                         \u001B[38;5;28;01mif\u001B[39;00m box[\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m>\u001B[39m MIN_CONFIDENCE \n\u001B[0;32m    152\u001B[0m                         \u001B[38;5;129;01mand\u001B[39;00m (box[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m-\u001B[39mbox[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m*\u001B[39m(box[\u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m-\u001B[39mbox[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1000\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:179\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, source, stream, **kwargs)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    152\u001B[0m     source: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28mint\u001B[39m, Image\u001B[38;5;241m.\u001B[39mImage, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray, torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    153\u001B[0m     stream: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    155\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m    156\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(source, stream, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:557\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, source, stream, predictor, **kwargs)\u001B[0m\n\u001B[0;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_prompts\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[0;32m    556\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mset_prompts(prompts)\n\u001B[1;32m--> 557\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mpredict_cli(source\u001B[38;5;241m=\u001B[39msource) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor(source\u001B[38;5;241m=\u001B[39msource, stream\u001B[38;5;241m=\u001B[39mstream)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:173\u001B[0m, in \u001B[0;36mBasePredictor.__call__\u001B[1;34m(self, source, model, stream, *args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m---> 36\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     40\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:231\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[1;34m(self, source, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_model(model)\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:  \u001B[38;5;66;03m# for thread-safe inference\u001B[39;00m\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;66;03m# Setup source every time predict is called\u001B[39;00m\n\u001B[1;32m--> 231\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_source(source \u001B[38;5;28;01mif\u001B[39;00m source \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msource)\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;66;03m# Check if save_dir/ label file exists\u001B[39;00m\n\u001B[0;32m    234\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msave \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msave_txt:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:203\u001B[0m, in \u001B[0;36mBasePredictor.setup_source\u001B[1;34m(self, source)\u001B[0m\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimgsz \u001B[38;5;241m=\u001B[39m check_imgsz(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mimgsz, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstride, min_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# check image size\u001B[39;00m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;28mgetattr\u001B[39m(\n\u001B[0;32m    196\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    202\u001B[0m )\n\u001B[1;32m--> 203\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset \u001B[38;5;241m=\u001B[39m load_inference_source(\n\u001B[0;32m    204\u001B[0m     source\u001B[38;5;241m=\u001B[39msource,\n\u001B[0;32m    205\u001B[0m     batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mbatch,\n\u001B[0;32m    206\u001B[0m     vid_stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mvid_stride,\n\u001B[0;32m    207\u001B[0m     buffer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mstream_buffer,\n\u001B[0;32m    208\u001B[0m )\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39msource_type\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_type\u001B[38;5;241m.\u001B[39mstream\n\u001B[0;32m    212\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_type\u001B[38;5;241m.\u001B[39mscreenshot\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1000\u001B[39m  \u001B[38;5;66;03m# many images\u001B[39;00m\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvideo_flag\u001B[39m\u001B[38;5;124m\"\u001B[39m, [\u001B[38;5;28;01mFalse\u001B[39;00m]))\n\u001B[0;32m    215\u001B[0m ):  \u001B[38;5;66;03m# videos\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\data\\build.py:192\u001B[0m, in \u001B[0;36mload_inference_source\u001B[1;34m(source, batch, vid_stride, buffer)\u001B[0m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Dataloader\u001B[39;00m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tensor:\n\u001B[1;32m--> 192\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m LoadTensor(source)\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m in_memory:\n\u001B[0;32m    194\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m source\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\data\\loaders.py:542\u001B[0m, in \u001B[0;36mLoadTensor.__init__\u001B[1;34m(self, im0)\u001B[0m\n\u001B[0;32m    540\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, im0) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    541\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Initialize LoadTensor object for processing torch.Tensor image data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_single_check(im0)\n\u001B[0;32m    543\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim0\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    544\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\data\\loaders.py:561\u001B[0m, in \u001B[0;36mLoadTensor._single_check\u001B[1;34m(im, stride)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m%\u001B[39m stride \u001B[38;5;129;01mor\u001B[39;00m im\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m%\u001B[39m stride:\n\u001B[0;32m    560\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(s)\n\u001B[1;32m--> 561\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39mfinfo(im\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39meps:  \u001B[38;5;66;03m# torch.float32 eps is 1.2e-07\u001B[39;00m\n\u001B[0;32m    562\u001B[0m     LOGGER\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    563\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWARNING ‚ö†Ô∏è torch.Tensor inputs should be normalized 0.0-1.0 but max value is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mim\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    564\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDividing input by 255.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    565\u001B[0m     )\n\u001B[0;32m    566\u001B[0m     im \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39mfloat() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
